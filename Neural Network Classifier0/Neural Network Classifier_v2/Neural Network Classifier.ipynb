{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b93b032-aa3a-4428-94c1-cd89493c11c1",
   "metadata": {},
   "source": [
    "# the version twoof our classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9e187d-63b0-41c2-b247-c692abf28b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, r2_score, mean_squared_error, mean_absolute_error,\n",
    "    mean_absolute_percentage_error, mean_squared_log_error\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_neural_network_classification(csv_file_path):\n",
    "    \"\"\"\n",
    "    Performs neural network binary classification on a dataset, calculates\n",
    "    various classification and regression metrics, generates a confusion\n",
    "    matrix heatmap, and saves all metrics to an Excel file.\n",
    "\n",
    "    This version includes an improved model architecture with Batch Normalization\n",
    "    and an additional hidden layer to prevent overfitting and optimize performance.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): The path to the CSV file. All columns except the last\n",
    "                             are treated as features (X), and the last column,\n",
    "                             which should contain 0s and 1s, is the target variable (y).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Separate features (X) and target (y)\n",
    "        X = df.iloc[:, :-1]  # All columns except the last\n",
    "        y = df.iloc[:, -1]   # The last column (0 or 1)\n",
    "\n",
    "        # Split the data into training and testing sets (80/20 split)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Standardize the data to help the neural network converge faster\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # --- Define Callbacks for Training ---\n",
    "        # EarlyStopping: Stop training when validation loss stops improving for a certain number of epochs.\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=50,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # ReduceLROnPlateau: Reduce the learning rate when a metric has stopped improving.\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=20,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "\n",
    "        # --- Improved Neural Network Model Setup ---\n",
    "        # Added Batch Normalization and a third hidden layer for a deeper network\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer and first hidden layer with L2 regularization\n",
    "        model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],),\n",
    "                        kernel_regularizer=l2(0.001)))\n",
    "        model.add(BatchNormalization()) # Added Batch Normalization layer\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        # Second hidden layer\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "        model.add(BatchNormalization()) # Added Batch Normalization layer\n",
    "        model.add(Dropout(0.3)) # Adjusted dropout rate slightly\n",
    "        \n",
    "        # Third hidden layer\n",
    "        model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "        model.add(BatchNormalization()) # Added Batch Normalization layer\n",
    "\n",
    "        # Output layer for binary classification\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model with the Adam optimizer and binary cross-entropy loss\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Print the model summary\n",
    "        print(\"Model Summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Train the model with the added callbacks\n",
    "        print(\"\\nTraining Neural Network model...\")\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=2000,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # --- Save the Trained Model ---\n",
    "        model_path = 'best_model.keras'\n",
    "        model.save(model_path)\n",
    "        print(f\"\\nModel saved successfully to '{model_path}'\")\n",
    "\n",
    "        # --- Make Predictions ---\n",
    "        y_pred_proba = model.predict(X_test_scaled).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "        # --- 1. Calculate Classification Metrics ---\n",
    "        print(\"\\n--- Neural Network Model Performance Metrics ---\")\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "        \n",
    "        # --- 2. Calculate Regression Metrics on Probabilities ---\n",
    "        print(\"\\n--- Regression Metrics on Predicted Probabilities ---\")\n",
    "\n",
    "        r2 = r2_score(y_test, y_pred_proba)\n",
    "        print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred_proba)\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred_proba)\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "        mape = np.mean(np.abs((y_test - y_pred_proba) / (y_test + 1e-8))) * 100\n",
    "        print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "        msle = mean_squared_log_error(y_test + 1e-8, y_pred_proba + 1e-8)\n",
    "        print(f\"Mean Squared Log Error (MSLE): {msle:.4f}\")\n",
    "\n",
    "        # --- 3. Save Metrics to Excel ---\n",
    "        metrics_data = {\n",
    "            'Metric': ['Accuracy', 'ROC AUC', 'R2 Score', 'MAE', 'MSE', 'RMSE', 'MAPE', 'MSLE'],\n",
    "            'Value': [accuracy, roc_auc, r2, mae, mse, rmse, mape, msle]\n",
    "        }\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        excel_path = 'nn_performance_metrics.xlsx'\n",
    "        metrics_df.to_excel(excel_path, index=False)\n",
    "        print(f\"\\nModel performance metrics saved to '{excel_path}'\")\n",
    "\n",
    "        # --- 4. Generate Confusion Matrix Plot ---\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                    yticklabels=['Actual 0', 'Actual 1'])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        \n",
    "        plot_path = 'confusion_matrix.svg'\n",
    "        plt.savefig(plot_path, format='svg')\n",
    "        print(f\"\\nConfusion matrix plot saved to '{plot_path}'\")\n",
    "        plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example Usage: Uncomment the line below and provide the path to your CSV file\n",
    "# if __name__ == \"__main__\":\n",
    "#     perform_neural_network_classification(\"path/to/your/data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d524fe34-6f70-4861-b978-32f5a280fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\River-Flood-Prediction\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,057</span> (51.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,057\u001b[0m (51.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,609</span> (49.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,609\u001b[0m (49.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network model...\n",
      "Epoch 1/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5044 - loss: 0.9099 - val_accuracy: 0.5144 - val_loss: 0.8421 - learning_rate: 0.0010\n",
      "Epoch 2/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5130 - loss: 0.8577 - val_accuracy: 0.4944 - val_loss: 0.8391 - learning_rate: 0.0010\n",
      "Epoch 3/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5127 - loss: 0.8463 - val_accuracy: 0.5088 - val_loss: 0.8293 - learning_rate: 0.0010\n",
      "Epoch 4/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5220 - loss: 0.8304 - val_accuracy: 0.5088 - val_loss: 0.8219 - learning_rate: 0.0010\n",
      "Epoch 5/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5209 - loss: 0.8180 - val_accuracy: 0.5281 - val_loss: 0.8123 - learning_rate: 0.0010\n",
      "Epoch 6/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5270 - loss: 0.8094 - val_accuracy: 0.5231 - val_loss: 0.8029 - learning_rate: 0.0010\n",
      "Epoch 7/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5358 - loss: 0.7972 - val_accuracy: 0.5038 - val_loss: 0.7974 - learning_rate: 0.0010\n",
      "Epoch 8/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5270 - loss: 0.7887 - val_accuracy: 0.5038 - val_loss: 0.7888 - learning_rate: 0.0010\n",
      "Epoch 9/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5373 - loss: 0.7814 - val_accuracy: 0.5163 - val_loss: 0.7827 - learning_rate: 0.0010\n",
      "Epoch 10/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5442 - loss: 0.7717 - val_accuracy: 0.5163 - val_loss: 0.7771 - learning_rate: 0.0010\n",
      "Epoch 11/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5464 - loss: 0.7646 - val_accuracy: 0.5188 - val_loss: 0.7709 - learning_rate: 0.0010\n",
      "Epoch 12/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5447 - loss: 0.7585 - val_accuracy: 0.5150 - val_loss: 0.7644 - learning_rate: 0.0010\n",
      "Epoch 13/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5344 - loss: 0.7547 - val_accuracy: 0.5163 - val_loss: 0.7594 - learning_rate: 0.0010\n",
      "Epoch 14/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5466 - loss: 0.7484 - val_accuracy: 0.5169 - val_loss: 0.7532 - learning_rate: 0.0010\n",
      "Epoch 15/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5470 - loss: 0.7433 - val_accuracy: 0.5075 - val_loss: 0.7507 - learning_rate: 0.0010\n",
      "Epoch 16/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5537 - loss: 0.7361 - val_accuracy: 0.5138 - val_loss: 0.7462 - learning_rate: 0.0010\n",
      "Epoch 17/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5516 - loss: 0.7325 - val_accuracy: 0.5031 - val_loss: 0.7436 - learning_rate: 0.0010\n",
      "Epoch 18/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5575 - loss: 0.7276 - val_accuracy: 0.5063 - val_loss: 0.7402 - learning_rate: 0.0010\n",
      "Epoch 19/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5528 - loss: 0.7252 - val_accuracy: 0.5063 - val_loss: 0.7355 - learning_rate: 0.0010\n",
      "Epoch 20/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5586 - loss: 0.7217 - val_accuracy: 0.5050 - val_loss: 0.7361 - learning_rate: 0.0010\n",
      "Epoch 21/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5584 - loss: 0.7179 - val_accuracy: 0.5144 - val_loss: 0.7338 - learning_rate: 0.0010\n",
      "Epoch 22/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5592 - loss: 0.7181 - val_accuracy: 0.5200 - val_loss: 0.7327 - learning_rate: 0.0010\n",
      "Epoch 23/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5577 - loss: 0.7158 - val_accuracy: 0.5169 - val_loss: 0.7302 - learning_rate: 0.0010\n",
      "Epoch 24/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5550 - loss: 0.7142 - val_accuracy: 0.5144 - val_loss: 0.7294 - learning_rate: 0.0010\n",
      "Epoch 25/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5650 - loss: 0.7104 - val_accuracy: 0.5106 - val_loss: 0.7267 - learning_rate: 0.0010\n",
      "Epoch 26/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5702 - loss: 0.7104 - val_accuracy: 0.5075 - val_loss: 0.7295 - learning_rate: 0.0010\n",
      "Epoch 27/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5634 - loss: 0.7080 - val_accuracy: 0.5019 - val_loss: 0.7294 - learning_rate: 0.0010\n",
      "Epoch 28/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5597 - loss: 0.7081 - val_accuracy: 0.5075 - val_loss: 0.7281 - learning_rate: 0.0010\n",
      "Epoch 29/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5591 - loss: 0.7061 - val_accuracy: 0.5063 - val_loss: 0.7260 - learning_rate: 0.0010\n",
      "Epoch 30/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5709 - loss: 0.7069 - val_accuracy: 0.5094 - val_loss: 0.7280 - learning_rate: 0.0010\n",
      "Epoch 31/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5678 - loss: 0.7063 - val_accuracy: 0.5175 - val_loss: 0.7252 - learning_rate: 0.0010\n",
      "Epoch 32/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5641 - loss: 0.7074 - val_accuracy: 0.5094 - val_loss: 0.7249 - learning_rate: 0.0010\n",
      "Epoch 33/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5628 - loss: 0.7039 - val_accuracy: 0.5094 - val_loss: 0.7224 - learning_rate: 0.0010\n",
      "Epoch 34/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5653 - loss: 0.7041 - val_accuracy: 0.4875 - val_loss: 0.7279 - learning_rate: 0.0010\n",
      "Epoch 35/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5689 - loss: 0.7016 - val_accuracy: 0.5131 - val_loss: 0.7261 - learning_rate: 0.0010\n",
      "Epoch 36/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5678 - loss: 0.6999 - val_accuracy: 0.5244 - val_loss: 0.7277 - learning_rate: 0.0010\n",
      "Epoch 37/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5691 - loss: 0.7013 - val_accuracy: 0.5175 - val_loss: 0.7223 - learning_rate: 0.0010\n",
      "Epoch 38/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5672 - loss: 0.7021 - val_accuracy: 0.5100 - val_loss: 0.7249 - learning_rate: 0.0010\n",
      "Epoch 39/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5691 - loss: 0.7004 - val_accuracy: 0.4988 - val_loss: 0.7274 - learning_rate: 0.0010\n",
      "Epoch 40/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5694 - loss: 0.7017 - val_accuracy: 0.5125 - val_loss: 0.7244 - learning_rate: 0.0010\n",
      "Epoch 41/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5672 - loss: 0.6999 - val_accuracy: 0.5006 - val_loss: 0.7241 - learning_rate: 0.0010\n",
      "Epoch 42/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5681 - loss: 0.6983 - val_accuracy: 0.5100 - val_loss: 0.7297 - learning_rate: 0.0010\n",
      "Epoch 43/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5742 - loss: 0.6969 - val_accuracy: 0.4919 - val_loss: 0.7263 - learning_rate: 0.0010\n",
      "Epoch 44/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5783 - loss: 0.6966 - val_accuracy: 0.5088 - val_loss: 0.7263 - learning_rate: 0.0010\n",
      "Epoch 45/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5744 - loss: 0.6986 - val_accuracy: 0.5094 - val_loss: 0.7228 - learning_rate: 0.0010\n",
      "Epoch 46/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5844 - loss: 0.6959 - val_accuracy: 0.5069 - val_loss: 0.7276 - learning_rate: 0.0010\n",
      "Epoch 47/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5708 - loss: 0.6966 - val_accuracy: 0.5106 - val_loss: 0.7255 - learning_rate: 0.0010\n",
      "Epoch 48/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5775 - loss: 0.6969 - val_accuracy: 0.5100 - val_loss: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 49/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5738 - loss: 0.6961 - val_accuracy: 0.4981 - val_loss: 0.7304 - learning_rate: 0.0010\n",
      "Epoch 50/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5833 - loss: 0.6952 - val_accuracy: 0.5056 - val_loss: 0.7311 - learning_rate: 0.0010\n",
      "Epoch 51/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5781 - loss: 0.6956 - val_accuracy: 0.5006 - val_loss: 0.7322 - learning_rate: 0.0010\n",
      "Epoch 52/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5752 - loss: 0.6959 - val_accuracy: 0.5044 - val_loss: 0.7325 - learning_rate: 0.0010\n",
      "Epoch 53/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5858 - loss: 0.6941 - val_accuracy: 0.5044 - val_loss: 0.7287 - learning_rate: 0.0010\n",
      "Epoch 54/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5955 - loss: 0.6900 - val_accuracy: 0.5056 - val_loss: 0.7309 - learning_rate: 2.0000e-04\n",
      "Epoch 55/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5981 - loss: 0.6851 - val_accuracy: 0.5069 - val_loss: 0.7326 - learning_rate: 2.0000e-04\n",
      "Epoch 56/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6000 - loss: 0.6843 - val_accuracy: 0.5025 - val_loss: 0.7347 - learning_rate: 2.0000e-04\n",
      "Epoch 57/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5994 - loss: 0.6820 - val_accuracy: 0.4969 - val_loss: 0.7361 - learning_rate: 2.0000e-04\n",
      "Epoch 58/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.6077 - loss: 0.6767 - val_accuracy: 0.4969 - val_loss: 0.7387 - learning_rate: 2.0000e-04\n",
      "Epoch 59/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6156 - loss: 0.6756 - val_accuracy: 0.4969 - val_loss: 0.7393 - learning_rate: 2.0000e-04\n",
      "Epoch 60/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6127 - loss: 0.6725 - val_accuracy: 0.4994 - val_loss: 0.7410 - learning_rate: 2.0000e-04\n",
      "Epoch 61/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6197 - loss: 0.6726 - val_accuracy: 0.5050 - val_loss: 0.7426 - learning_rate: 2.0000e-04\n",
      "Epoch 62/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6087 - loss: 0.6705 - val_accuracy: 0.5013 - val_loss: 0.7448 - learning_rate: 2.0000e-04\n",
      "Epoch 63/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6117 - loss: 0.6725 - val_accuracy: 0.5069 - val_loss: 0.7438 - learning_rate: 2.0000e-04\n",
      "Epoch 64/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6186 - loss: 0.6719 - val_accuracy: 0.5106 - val_loss: 0.7428 - learning_rate: 2.0000e-04\n",
      "Epoch 65/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6245 - loss: 0.6663 - val_accuracy: 0.5081 - val_loss: 0.7439 - learning_rate: 2.0000e-04\n",
      "Epoch 66/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6194 - loss: 0.6672 - val_accuracy: 0.4994 - val_loss: 0.7458 - learning_rate: 2.0000e-04\n",
      "Epoch 67/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6070 - loss: 0.6749 - val_accuracy: 0.5006 - val_loss: 0.7440 - learning_rate: 2.0000e-04\n",
      "Epoch 68/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6206 - loss: 0.6684 - val_accuracy: 0.5069 - val_loss: 0.7439 - learning_rate: 2.0000e-04\n",
      "Epoch 69/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6162 - loss: 0.6691 - val_accuracy: 0.5013 - val_loss: 0.7460 - learning_rate: 2.0000e-04\n",
      "Epoch 70/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6266 - loss: 0.6641 - val_accuracy: 0.5013 - val_loss: 0.7447 - learning_rate: 2.0000e-04\n",
      "Epoch 71/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6112 - loss: 0.6675 - val_accuracy: 0.5056 - val_loss: 0.7454 - learning_rate: 2.0000e-04\n",
      "Epoch 72/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6283 - loss: 0.6619 - val_accuracy: 0.5013 - val_loss: 0.7493 - learning_rate: 2.0000e-04\n",
      "Epoch 73/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6233 - loss: 0.6651 - val_accuracy: 0.4981 - val_loss: 0.7478 - learning_rate: 2.0000e-04\n",
      "Epoch 74/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6234 - loss: 0.6597 - val_accuracy: 0.4956 - val_loss: 0.7488 - learning_rate: 4.0000e-05\n",
      "Epoch 75/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6292 - loss: 0.6566 - val_accuracy: 0.4981 - val_loss: 0.7494 - learning_rate: 4.0000e-05\n",
      "Epoch 76/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6331 - loss: 0.6575 - val_accuracy: 0.5025 - val_loss: 0.7490 - learning_rate: 4.0000e-05\n",
      "Epoch 77/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6384 - loss: 0.6568 - val_accuracy: 0.5031 - val_loss: 0.7495 - learning_rate: 4.0000e-05\n",
      "Epoch 78/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6370 - loss: 0.6562 - val_accuracy: 0.5006 - val_loss: 0.7508 - learning_rate: 4.0000e-05\n",
      "Epoch 79/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6416 - loss: 0.6556 - val_accuracy: 0.4963 - val_loss: 0.7524 - learning_rate: 4.0000e-05\n",
      "Epoch 80/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6272 - loss: 0.6644 - val_accuracy: 0.4956 - val_loss: 0.7523 - learning_rate: 4.0000e-05\n",
      "Epoch 81/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6309 - loss: 0.6564 - val_accuracy: 0.4994 - val_loss: 0.7524 - learning_rate: 4.0000e-05\n",
      "Epoch 82/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6269 - loss: 0.6591 - val_accuracy: 0.5031 - val_loss: 0.7510 - learning_rate: 4.0000e-05\n",
      "Epoch 83/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6278 - loss: 0.6583 - val_accuracy: 0.5019 - val_loss: 0.7514 - learning_rate: 4.0000e-05\n",
      "Epoch 84/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6320 - loss: 0.6577 - val_accuracy: 0.4994 - val_loss: 0.7507 - learning_rate: 4.0000e-05\n",
      "Epoch 85/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6305 - loss: 0.6589 - val_accuracy: 0.5013 - val_loss: 0.7517 - learning_rate: 4.0000e-05\n",
      "Epoch 86/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6280 - loss: 0.6601 - val_accuracy: 0.5056 - val_loss: 0.7519 - learning_rate: 4.0000e-05\n",
      "Epoch 87/2000\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6289 - loss: 0.6571 - val_accuracy: 0.5038 - val_loss: 0.7522 - learning_rate: 4.0000e-05\n",
      "\n",
      "Model saved successfully to 'best_model.keras'\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "--- Neural Network Model Performance Metrics ---\n",
      "Accuracy: 0.5055\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.48      0.49       989\n",
      "         1.0       0.51      0.54      0.52      1011\n",
      "\n",
      "    accuracy                           0.51      2000\n",
      "   macro avg       0.51      0.51      0.50      2000\n",
      "weighted avg       0.51      0.51      0.51      2000\n",
      "\n",
      "ROC AUC Score: 0.5105\n",
      "\n",
      "--- Regression Metrics on Predicted Probabilities ---\n",
      "R-squared (R2): -0.0238\n",
      "Mean Absolute Error (MAE): 0.4986\n",
      "Mean Squared Error (MSE): 0.2559\n",
      "Root Mean Squared Error (RMSE): 0.5059\n",
      "Mean Absolute Percentage Error (MAPE): 2472222176.45%\n",
      "Mean Squared Log Error (MSLE): 0.1258\n",
      "\n",
      "Model performance metrics saved to 'nn_performance_metrics.xlsx'\n",
      "\n",
      "Confusion matrix plot saved to 'confusion_matrix.svg'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_neural_network_classification(\"../../mapped_dataset_Normalized_version.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a1844-cf79-4cb9-ac68-192bc5e2babd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
